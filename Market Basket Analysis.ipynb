{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "suVO4RPhxOkI"
   },
   "source": [
    "<div id=\"container\" style=\"position:relative;\">\n",
    "<div style=\"float:left\"><h1> Market Basket Analysis </h1></div>\n",
    "<div style=\"position:relative; float:right\"><img style=\"height:65px\" src =\"https://drive.google.com/uc?export=view&id=1EnB0x-fdqMp6I5iMoEBBEuxB_s7AmE2k\" />\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qid50vzNxOkJ"
   },
   "source": [
    "So far, we have seen supervised learning, and unsupervised learning. Another kind of learning is *rules-based* learning, which debatedly belongs to the machine learning family. We will also briefly cover another type, reinforcement learning, in the future.\n",
    "\n",
    "In rules-based learning, rather than learning labels or clustering like in machine learning, or relationships like in regressions, we are trying to learn rules of associations between objects.\n",
    "\n",
    "A rule might look like: '*If* a customer bought a plane ticket, *then* they will buy a hotel room.' This example might be obvious, but a (possibly apocryphal) association rule:\n",
    "\n",
    "*If* the customer is male, aged 20-40, and buys diapers between 5 and 7pm, *then* customer will also buy beer.\n",
    "\n",
    "Mining retail datasets like this is done to find a number of relations:\n",
    "\n",
    "* **Complementary products**: products which are often bought together, like chips and salsa\n",
    "* **Substitute products**: products which replace each other, like Coke and Pepsi\n",
    "* **Trigger products**: products which when bought, trigger other purchases \n",
    "* **Common Baskets**: combinations of products that are often bought together\n",
    "\n",
    "This kind of data is gold to retailers: We can design promos where one complement is discounted and the rest of the items are marked up, offer discounts for commonly bought items, plan store layout, recommend items, and promote cross-sell and upsell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTywoBnFxOkK"
   },
   "source": [
    "### Coffee Preferences\n",
    "\n",
    "We have set up a store which only sells three items: Coffee, Milk and Sugar. Our basket types are thus all combinations between the three items.\n",
    "\n",
    "We have a dataset of 'baskets' - you can download [the data from here](https://drive.google.com/uc?export=view&id=1iI1IJZXlC0WgcSzQv40vl2fpXm-22aW-). Each transaction that comes through our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>916369</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>916369</td>\n",
       "      <td>milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>916369</td>\n",
       "      <td>sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>743789</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>743789</td>\n",
       "      <td>milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>743789</td>\n",
       "      <td>sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>169588</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>169588</td>\n",
       "      <td>milk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>169588</td>\n",
       "      <td>sugar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>723327</td>\n",
       "      <td>coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_id    item\n",
       "0          916369  coffee\n",
       "1          916369    milk\n",
       "2          916369   sugar\n",
       "3          743789  coffee\n",
       "4          743789    milk\n",
       "5          743789   sugar\n",
       "6          169588  coffee\n",
       "7          169588    milk\n",
       "8          169588   sugar\n",
       "9          723327  coffee"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "basket_df = pd.read_csv(\"C:/Users/Surface/Downloads/baskets.csv\")\n",
    "basket_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataframe, every item that was purchased has its transcation id associated with it, but we would like to group items into complete transcations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 1\n",
    "\n",
    "Group the dataframe into a pandas series so that each transaction id indexes a list of the items purchased with it.\n",
    "\n",
    "e.g.:\n",
    "\n",
    "```\n",
    "transaction_id\n",
    "2376      [coffee, milk, sugar]\n",
    "3688                   [coffee]\n",
    "10266            [coffee, milk]\n",
    "26740     [coffee, milk, sugar]\n",
    "40073     [coffee, milk, sugar]\n",
    "                  ...          \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a series of lists where the index of every list is its transaction id. We don't care about the transaction ids too much so we will just grab the values of that series. The values come back as a numpy array of lists (**NOTE: This is not the same as a 2D numpy array**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_baskets_series' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m my_baskets \u001b[38;5;241m=\u001b[39m \u001b[43mmy_baskets_series\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      2\u001b[0m my_baskets\n",
      "\u001b[1;31mNameError\u001b[0m: name 'my_baskets_series' is not defined"
     ]
    }
   ],
   "source": [
    "my_baskets = my_baskets_series.values\n",
    "my_baskets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OrRiV0m-xOkQ"
   },
   "source": [
    "This is not nice data! We can't just put it into a dataframe - there is a different number of items bought each time.\n",
    "\n",
    "In R, the standard package to analyse this type of data is `arules`, but it is not very user-friendly. As we have been working with `scikit-learn`, let's stick to the same ecosystem.\n",
    "\n",
    "Unfortunately, `scikit-learn` does not have anything built in either - but we can use the `mlxtend` package, which contains a number of extensions to `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbDwcu10xOkT"
   },
   "source": [
    "### Using mlxtend\n",
    "\n",
    "We follow the same basic API in `mlxtend` as in `scikit-learn`. We have preprocessing, along with models that we use to fit, transform and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJVD9Gr1xOkU",
    "outputId": "a86c52b7-271d-48cc-d505-514325276b5b"
   },
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJVD9Gr1xOkU",
    "outputId": "a86c52b7-271d-48cc-d505-514325276b5b"
   },
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "coffee = te.fit_transform(my_baskets)\n",
    "coffee_df = pd.DataFrame(coffee, columns=te.columns_)\n",
    "coffee_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y97P2lU-xOka"
   },
   "source": [
    "Great, so we now have a data frame with columns as dummy variables for each item in our store. The rows are each a basket. In association rules, we normally discount the total number of an item bought - though it is possible to add a 'fake item' flag to tag larger purchases of items.\n",
    "\n",
    "What if we have a large store/dataset? Our columns might number in the tens of thousands. Amazon sells over 500 million distinct items. We will discuss solutions shortly - for now let's keep working on our coffee shops data.\n",
    "\n",
    "First, we can do some simple exploratory data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "###How many in total:\n",
    "print(coffee_df.sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a co-occurrence table:\n",
    "co_occurrence = pd.DataFrame({\"coffee\": [0,0,0],\n",
    "                             \"milk\": [0,0,0],\n",
    "                             \"sugar\": [0,0,0]},\n",
    "                            index = [\"coffee\", \"milk\", \"sugar\"])\n",
    "co_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row\n",
    "for index, row in coffee_df.iterrows():\n",
    "    # For each item combination\n",
    "    for item1 in [\"coffee\", \"milk\", \"sugar\"]:\n",
    "        for item2 in [\"coffee\", \"milk\", \"sugar\"]:\n",
    "            # If both are true, add one to the associated index in the co-occurence table\n",
    "            if row[item1] and row[item2]:\n",
    "                co_occurrence.loc[item1, item2] += 1\n",
    "co_occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5LqvkpHOxOkb",
    "outputId": "b3b5fff5-d7c1-47ee-bb94-13ea68e865f0"
   },
   "outputs": [],
   "source": [
    "# Turn the table into percent\n",
    "co_occurrence/coffee_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also be accomplished with some linear algebra magic\n",
    "ctable = coffee_df.T.dot(coffee_df.astype('int'))\n",
    "ctable = ctable/coffee_df.shape[0]\n",
    "ctable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVfelUHIxOke"
   },
   "source": [
    "By eyeballing the data, we can see that the coffee is most common, milk is less common, and sugar is the least common. It is hard to tell if there are any rules about which products co-occur in purchases.\n",
    "\n",
    "This is where association rules come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5dErc37xOkf"
   },
   "source": [
    "### Apriori Algorithm\n",
    "\n",
    "Again, the problem of a large number of items rears its head. What we want to do is to create all possible combinations of items, then see which items are most commonly also purchased, given that one of these combinations has been purchased.\n",
    "\n",
    "We can see that for a number of objects, $n$, in our store, there are about $n^x$ possible $x$ sized baskets\n",
    "\n",
    "The method for creating our baskets is called the Apriori algorithm (Agrawal & Srikant, 1994<sup>[1](http://www.vldb.org/conf/1994/P487.PDF)</sup>). There are several other more efficient methods since proposed but not coded, so we will stick with it for now. [Wikipedia has the exact details](https://en.wikipedia.org/wiki/Apriori_algorithm).\n",
    "\n",
    "The idea is, we take a threshold occurrence, $C$, and find all individual items with occurence greater than $C$. Any items that are less than our threshold are removed from further analysis. We then go up a level and find all _pairs_ of non-excluded objects, and use the same threshold to exclude items. We can progressively widen the number of items in our sets while avoiding some of the explosion of computation size with sensible exclusions.\n",
    "\n",
    "This is implemented in mlxtend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FpLQnXltxOkg",
    "outputId": "6afcb2a9-f35b-4664-9b4d-25487abab548"
   },
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "### create a df with all our items: threshold, names and len\n",
    "apriori(coffee_df, min_support=0.5, use_colnames=True, max_len = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xaL76CfWxOkj"
   },
   "source": [
    "We can see the *support* score is the proportion of baskets that contain the given item, or combinations of items.\n",
    "\n",
    "In the case here with setting a minimum support of 0.5, all of our combinations are returned. In a larger example, we would start to find interesting co-occurrences here.\n",
    "\n",
    "The output is a pandas dataframe, so we can filter, sort, *etc.* as desired. The `itemsets` column is of object datatype, and contains tuples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vPW-0CNxOkk",
    "outputId": "536afc64-3fa1-4033-a48e-e0389e1b729e"
   },
   "outputs": [],
   "source": [
    "x = apriori(coffee_df, min_support=0.5, use_colnames=True, max_len = 3)\n",
    "x['length'] = x.itemsets.apply(lambda x: len(x))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuq8Es_YxOko"
   },
   "source": [
    "### Determining Rules\n",
    "\n",
    "Once our data is in the format above, we can begin to determine association rules.\n",
    "\n",
    "Here, we calculate several metrics to analyse the rules. These are calculated automatically by the package, but we will take time to understand them.\n",
    "\n",
    "First, all of our groups are designated as 'antecedents' and 'consequents'. This allows us to say: 'given this group of antecedents, we see this group of consequents with frequency x'. We will designate antecedents as $X$ and consequents as $Y$ below.\n",
    "\n",
    "Let's make some rules for illustration of these measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C2d_oFrhN8z7",
    "outputId": "f27c361c-f5a9-497e-c5cf-472a73c37eb6"
   },
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "x = apriori(coffee_df, min_support=0.5, use_colnames=True)\n",
    "\n",
    "#take a look at the help for ways we can use this function\n",
    "association_rules(x, metric=\"lift\", min_threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHyR5eukxOkv"
   },
   "source": [
    "We have already calculated **support**: How often our items, or group of items occur in the dataset.\n",
    "\n",
    "$$  \\text{support}(X \\cup Y) = \\frac{\\text{# of transactions with X and Y together}}{\\text{total number of transactions}} $$\n",
    "\n",
    "If items are not related, we would expect support of one to be independent of the support of the other. If item $X$, our antecedent, occurs in 0.7 of baskets, and item $Y$, our consequent, occurs in 0.6, we expect them to occur together in 0.7 * 0.6 = 0.42 (or 42%) of baskets. \n",
    "\n",
    "If the proportion is higher, then we have items which are occurring at a higher frequency than expected - this might indicate that we have a useful association rule. If `milk`, then `sugar`.\n",
    "\n",
    "**Confidence** is a measure of how much more likely one basket is to occur than another. It is calculated by dividing the support of our antecedent and consequent together by the support of our antecedent alone:\n",
    "\n",
    "$$ \\text{confidence}(X\\rightarrow Y) = \\frac{\\text{support}(X\\cup Y)}{\\text{support}(X)} = \\frac{\\text{proportion of transactions with X and Y}}{\\text{proportion of transactions with X}}$$\n",
    "\n",
    "If $X$ and $Y$ are unrelated, we expect $\\text{support}(X \\cup Y) = \\text{support}(X) \\times \\text{support}(Y)$, so a value of 1 indicates that our consequent is always bought with our antecedent. If $\\text{confidence}(X\\rightarrow Y) = \\text{support}(Y)$ this suggests no relationship between the two, and a lower value suggests that they are substitutes.\n",
    "\n",
    "**Lift** measures a similar idea: How much we have _lifted_ the purchase likelihood of the consequent by having antecedent included in our basket. A values of 1 represents no increase.\n",
    "\n",
    "$$ \\text{lift}(X\\rightarrow Y) = \\frac{\\text{confidence}(X\\rightarrow Y)}{\\text{support}(Y)} = \\frac{\\frac{\\text{support}(X\\cup Y)}{\\text{support}(X)}}{\\text{support}(Y)} = \\frac{\\text{support}(X\\cup Y)}{\\text{support}(X)\\times\\text{support}(Y)}$$\n",
    "\n",
    "We can think of lift as measuring how much more often $X$ and $Y$ occur together than expected if their purchase frequency were independent.\n",
    "\n",
    "**Leverage** is the difference in support of the larger group, than would be expected if the antecedent and consequent were independent:\n",
    "\n",
    "$$\\text{leverage}(X\\rightarrow Y) = \\text{support}(X\\cup Y) - \\text{support}(X) \\times \\text{support}(Y)$$\n",
    "\n",
    "**Conviction** is a measure of the dependence of the consequent on the antecedent. It compares the expected proportion of $X$ appearing without $Y$ if they were dependent with the actual proportion of baskets containing $X$ without $Y$. A high value denotes that we always see the consequent purchased with the antecedent:\n",
    "\n",
    "$$\\text{conviction}(X\\rightarrow Y) = \\frac{1 - \\text{support}(Y)}{1 - \\text{confidence}(X\\rightarrow Y)} = \\frac{\\text{proportion with $X$}\\times\\text{proportion without $Y$}}{\\text{proportion with $X$ and without $Y$}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IM0_l_YxOkw"
   },
   "source": [
    "### How to interpret\n",
    "\n",
    "We have a range of metrics - how to we decide which to report, and what is significant?\n",
    "\n",
    "We would normally report the support, lift and confidence. \n",
    "\n",
    "**Support** allows us to see how often the basket occurs. We don't want to waste our time promoting strong links between items if only a few people buy them.\n",
    "\n",
    "**Confidence** allows us to see the strength of the rule. What proportion of transactions with our first item also contain the other item (or items)?\n",
    "\n",
    "**Lift** can be interpreted a measure of how much we potentially drive up the sales of the consequent by the relationship? In theory it can be seen as proportional to the increase of sales of the antecedent.\n",
    "\n",
    "In practice, we would start with all rules with lift above 1, and drill down into the pricing, sales, and desires of our store. \n",
    "\n",
    "---\n",
    "Additional Association Rules: Leverage and Conviction are less common options for assessing the strength of the co-occurrence relationship.\n",
    "\n",
    "**Leverage** computes the difference between the observed frequency of X and Y appearing together and the frequency that would be expected if X and Y were independent. A leverage value of 0 indicates independence.\n",
    "\n",
    "The rationale in a sales setting is to find out how many more units (items X and Y together) are sold than expected from the independent sales.\n",
    "\n",
    "**Conviction** looks at the ratio of the expected frequency that the rule makes an incorrect prediction if X and Y were independent, divided by the observed frequency of incorrect predictions.\n",
    "\n",
    "If the conviction value is greater than 1, then incorrect predictions occur less often compared to if these two actions were independent. A conviction of 1.5 for example, would indicate that if the variables were independent, the prediction would be incorrect 50% more often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SLbLa9pOxOk1"
   },
   "source": [
    "### Caveats\n",
    "\n",
    "We can see from our table above that lift and leverage are reversible, whereas conviction and confidence are not (*i.e.* $\\text{lift}(X\\rightarrow Y) = \\text{lift}(Y\\rightarrow X)$, however $\\text{confidence}(X\\rightarrow Y) \\neq \\text{confidence}(Y\\rightarrow X)$)\n",
    "\n",
    "We need to be careful about inferring causation from lift or leverage: we cannot say that the lift in $X$ or $Y$ was caused by $X$ or $Y$, just whether they more frequently occur together than chance or not.\n",
    "\n",
    "Similarly, confidence needs to be taken carefully. If we have two baskets with kiwis and diamonds, and two with just diamonds, our confidence for kiwi $\\rightarrow$ diamonds is 1! Our confidence for diamonds to kiwi is 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTEA1asQxOk1"
   },
   "source": [
    "### Working with larger data\n",
    "\n",
    "Creating the groups of all items is extremely expensive in larger sizes, and this is a constant problem in association rules.\n",
    "\n",
    "We could pool by product category - if all we want to predict is what categories of items go together, we could pool all game consoles, cables of a given type, or pastas. \n",
    "\n",
    "We could also run our model for each subcategory independently - when predicting rules for pasta, we could turn all basket items that are non-pasta into categories, and see if I bought a cheese item, I will buy rigatoni.\n",
    "\n",
    "One piece that can help is working with sparse matrices, which is implemented in this package. As well as scipy's sparse module, pandas has limited support for sparse matrices.\n",
    "\n",
    "In general, it is much more efficient to work in scipy and call `todense()` or `toarray()` when needed. However we can take advantage of Pandas DataFrame functionality to make easy to use dataframes with column names using the `DataFrame.sparse.from_spmatrix()` method. However, this will still consume more memory than using a pure scipy sparse matrix so be careful when you have large datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the TransactionEncoder and transform the buckets\n",
    "te = TransactionEncoder()\n",
    "coffee = te.fit_transform(my_baskets, sparse=True)#.transform(baskets, sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(coffee)\n",
    "print('')\n",
    "display(type(coffee))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "coffee_df = pd.DataFrame.sparse.from_spmatrix(coffee, columns=te.columns_)\n",
    "coffee_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the columns are using the Pandas Sparse dtypes. These are not exactly eqivalent to a standard scipy sparse matrix, however do help to reduce memory usage somewhat. You can read more about Pandas sparse data structures [here](https://pandas.pydata.org/docs/user_guide/sparse.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QulrDxDkxOk5"
   },
   "source": [
    "### Recommender Systems\n",
    "\n",
    "Once we have our rules, we can start to recommend items to customers. If we have the current basket, we can check our association rules for the next most common item, based on highest lift (or, highest proft for our store). Stores like Amazon can use association rules to efficiently recommend the next $n$ items that we might purchase, based on lift, confidence, expected profit, or past purchases.\n",
    "\n",
    "In reality, most current recommender systems will use a combination of approaches (*e.g.* collaborative filtering, clustering, *etc.*) depending on their requirements and intent. Association rules can be used for promotions, product placement, and coupons, as they take considerable time to generate and are specifically well-suited for these applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bc-QYK-WxOk6"
   },
   "source": [
    "---\n",
    "#### Exercise 2\n",
    "\n",
    "Download the `Online Retail.csv` [here](https://drive.google.com/uc?export=view&id=1r2OjAHs6C27Z7Z59mex_XNZ5lbcsisp9). We've provided you with the code to read in the data, and transform it into a sparse DataFrame. Use the resulting DataFrame for this exercise:\n",
    "\n",
    "1. Use the apriori algorithm (use min_support ~0.02) to reduce the dataset and then create association rules. Feel free to play around with different metrics to use as the threshold. \n",
    "\n",
    "2. Sort your association rules dataframe by one of the metrics in descending order. Try looking up the product descriptions for the antecedent and consequent stockcodes for the first row in your rules dataframe and think of a reason behind this purchasing pattern. \n",
    "\n",
    "2. Complete the function listed in a cell below. The function takes in a basket of items (stock codes), an association rules dataframe, and a specific association rules metric. The function will return a recommended item to purchase based on an item in the basket (Just like on Amazon!). Feel free to try it with various item combinations. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvoiceNo\n",
       "536365    [85123A, 71053, 84406B, 84029G, 84029E, 22752,...\n",
       "536366                                       [22633, 22632]\n",
       "536367    [84879, 22745, 22748, 22749, 22310, 84969, 226...\n",
       "536368                         [22960, 22913, 22912, 22914]\n",
       "536369                                              [21756]\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "retail_df = pd.read_csv(\"C:/Users/Surface/Downloads/Online Retail.csv\")\n",
    "\n",
    "# Create our list of 'baskets' for use with the TransactionEncoder\n",
    "basket_series = retail_df.groupby('InvoiceNo').apply(lambda x: list(x['StockCode']))\n",
    "basket_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CODE FIRST FOR Q3\n",
    "rules_df['antecedents'] = rules_df['antecedents'].apply(lambda x:list(x)).copy()\n",
    "rules_df['consequents'] = rules_df['consequents'].apply(lambda x:list(x)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Input basket\n",
    "mybasket = ['22698', '22697', '22356']\n",
    "\n",
    "#metric\n",
    "metric = 'lift'\n",
    "\n",
    "#COMPLETE THIS FUNCTION\n",
    "def product_recs(basket, rule_df, metric):\n",
    "    \n",
    "    # Randomly select an item from the basket\n",
    "    random_item = np.random.choice(basket, 1)[0]\n",
    "    print(random_item)\n",
    "    \n",
    "    # Find rules where the item is in the antecedent\n",
    "    \n",
    "    \n",
    "    # Filter the dataframe using rule_filter and sort by the selected metric\n",
    "   \n",
    "    \n",
    "    # Randomly return one of the top 20 items from the filtered dataframe\n",
    "  \n",
    "    \n",
    "    return reco\n",
    "\n",
    "product_recs(mybasket, rules_df, metric )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqlFwCaVxOk8"
   },
   "source": [
    "<div id=\"container\" style=\"position:relative;\">\n",
    "<div style=\"position:relative; float:right\"><img style=\"height:25px\"\"width: 50px\" src =\"https://drive.google.com/uc?export=view&id=14VoXUJftgptWtdNhtNYVm6cjVmEWpki1\" />\n",
    "</div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Market Basket Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
